---
title: "Webscraping and Parallel Processing"
author: "XXXXXX"
date: "November 20, 2017"
output: html_document
---

#Introduction 
At the end of our discussion about regular expressions, we introduced the concept of web scraping. Not all online data is in a tidy, downloadable format such as a .csv or .rda file. Yet, patterns in the underlying html code and regular expressions together provide a valuable way to "scrape" data off of a webpage. Here, we're going to give a slightly more complicated example of web scraping. 

As a preliminary matter, some R packages, such as rvest, can help with web scraping. Nonetheless, these packages sometimes do not have the flexibility that the below web scraping methods have.

First, you will need to make sure that you can access the underlying html code for the webpage that you want to scrape. If you're using Firefox, you can simply right click on a webpage and then click "View Page Source."  If you're using Microsoft Edge, you can right click on the webpage, click "View Source" and then look at the "Debugger" tab. 

Let's go to the webpage for the following url: http://www.the-numbers.com/box-office-chart/daily/2016/07/04

This page contains information about the movies that were shown in theaters on July 4, 2016 and the amount of money (in dollars) that each of those movies grossed that day. 

Make sure you can see the html code for this page using the methods described above.  Next, scan the html code into R; as you can see, the url of the webpage that you want to scrape goes within the quotes. `scan`, which we have used before, is a basic function that reads in text. For many websites the following method will scan the html code into R:

```{r comment="", results='hold'} 
a <- scan("http://www.the-numbers.com/box-office-chart/daily/2016/07/04",what="",sep="\n")
a[1:200]
```

However, for other websites you may have to use the httr package. Consider the following example:

```{r comment="", results='hold'}
library(httr)
resp <- GET("https://stores.org/stores-top-retailers-2017/")
a1 <- content(resp, as="text")
a1 <- strsplit(a1,"\n")[[1]]
```

Also, Mac users often encounter snags with the scan() command. For example, you might get "403 Forbidden" errors.  If so, use this approach:

```{r comment="",results='hold'}
library(httr) # this goes at the top
resp <- GET(url.text, user_agent("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.13+ (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2"))
a <- content(resp, as="text")
a <- strsplit(a,"\n")[[1]]
```

#Scraping one page

Let's try our luck at scraping one page.  As mentioned above, regular expressions will be indispensible to this process. We've already scanned in the html code for July 4, 2016 and assigned it the variable name "a." 

Go back to the html code. As you can see, there's a lot of material to search through in the html code - including the code for any advertisements that appear on a webpage. There is a lot of junk. So how do we throw out this junk without getting rid of valuable data?

In the html code, search for a movie name that appears on the webpage. Start with "The Purge: Election Year" as an example. That occurs on line 255 of the html code. We're trying to look for a pattern of characters and/or letters that always precedes any movie name in the html code.  Try another movie name such as "X Men: Apocalypse." That occurs on line 363 of the html code. In both instances, we can conclude that "movie" seems to precede each movie name in the html code. 

We next want to grep, from a, all of the lines in which a movie name occurs. The variable "i" is an index variable that gives us every line where a movie name appears. 
```{r comment="", results='hold'}
i <- grep("movie",a)
i
```
These numbers may not necessarily correspond to the numbering in the html code. For example, according to R, "The Purge: Election Year" and "X Men: Apocalypse" occur on lines 257 and 363, respectively. This discrepancy is not problematic - just something to be aware of. 

Next, run the following:
```{r comment="", results='hold'}
a[i]
```

We will now have a list of movies that played in theaters on July 4, 2016.  However, as you can see, we have a lot of excess symbols and html code to eliminate before we can have a neat list of movie names. 

We can gsub this "junk" out. But, what happens if we try the following? 

```{r comment="", results='hold'}
gsub("<[^>]*>","",a[i])

```

We've run into a bit of a problem. You also now have some column names in items 1-15 - besides movie names. If you look at the html code, you'll notice that the word "movie" also appears in lines (such as column names) other than those that contain movie names. 

There are two options: (1) you could use a phrase in the html code that uniquely precedes any line with a movie name - other than "movie"; or (2) you can limit the section of the html code that the word "movie" is grepped from. 

First method (find a different phrase):

Take a look at the html code again. You'll notice that "</a></b" uniquely occurs in every line with a movie name.  You can therefore do the following:
```{r comment="", results='hold'}
x <- grep("</a></b", a)
a[x]
gsub("<[^>]*>","",a[x])
```

Second method (limit the section of the html code that the word "movie" is grepped from)

Take a look at the html code again. You'll notice that the section of the html code that contains all the movie names starts with the phrase "<TABLE" and ends with the phrase "</table" - we can use this characteristic to limit what part of the html code we grep the word "movie" from.  We'll use a "conditional" to help us do this.

Run the following lines of code.  We use the [2] in the second line because there are two instances of "</table" in the html code; the first instance of "</table" occurs earlier in the html code in a part that we're not interested in; but, the second instance forms the "bottom" boundary of the movie list that we want.
```{r comment="", results='hold'}
i.table.start <- grep("<TABLE",a)
i.table.end   <- grep("</table",a)[2]
```

If more than five lines exist between i.table.start and i.table.end, then we know we're in the section of the html code that, if a line contains "movie", then it must be a line with a movie name. 

```{r comment="", results='hold'}

if(i.table.end-i.table.start > 5)
{
  # find movies
  i <- grep("movie",a)
  i <- i[i>i.table.start & i<i.table.end]
  a[i]
  gsub("<[^>]*>","",a[i])
}
```

We can then put the names of these movies in a dataframe, `data0`. This dataframe currently has only one column. 

```{r comment="", results='hold'}
data0 <- data.frame(movie=gsub("<[^>]*>","",a[i]))
data0
```

Let's add a "gross" column to the dataframe. Look at the html code. You'll notice that the gross information falls two lines below where the movie name is. Therefore, instead of i, we'll use i+2. Also, we'll remove the dollar signs and make gross numeric. That way, we will be able to sum the grosses or do any other quantitative analysis we want.

```{r comment="", results='hold'}
data0$gross <- gsub("<[^>]*>","",a[i+2])
data0$gross <- as.numeric(gsub("[$,]","",data0$gross))
```

Take a look at the webpage and compare it to the dataset you've now created. Do all the values match?  

#Scraping Multiple Pages

We've successfully scraped data for just one day.  But, now we want to web scrape from the numbers.com site: (1) each "daily" page showing the daily movie gross data for every day (2) that occurred on or between January 1, 2010 and October 1, 2016. That means we're going to be web scraping well over 2,000 pages of data.  As a result, we'll have to make use of for-loops. 

Before we go any further, however, we first need to create a "date-based" dataframe where we'll place our web scraped data. First, use R (and the lubridate package) to generate a sequence of dates.

```{r comment="", results='hold'}
library(lubridate)
dates.list <- data.frame(date=seq(ymd("2010-01-01"),ymd("2016-10-01"),by="days"))
dates.list$years  <- year(dates.list$date)
dates.list$months <- sprintf("%02d",month(dates.list$date))
dates.list$days   <- sprintf("%02d",day(dates.list$date))
```

We're going to use a vector of data frames to collect the results. Therefore results[[i]] will contain the movie data for date i.The following for-loop will likely take more than ten minutes to run. To that extent, before running the entire for-loop, it may be a good idea to temporarily set the dates to a short period of time (e.g., a month or two) just to verify that your code is functioning properly. Once you've concluded that the code is doing what you want it to do, you can set the dates so that the for-loop runs for the entire 6+ year period.  

```{r comment="", results='hold'}

results <- vector("list",nrow(dates.list))

for(i.date in 1:nrow(dates.list))
{
  # print out our progress
  with(dates.list, cat(years[i.date],"/",
                       months[i.date],"/",
                       days[i.date],"\n",sep=""))
  
  url.text <- paste("http://www.the-numbers.com/box-office-chart/daily/",
                    dates.list$years[i.date],"/",
                    dates.list$months[i.date],"/",
                    dates.list$days[i.date],
                    sep="")
  #note that we used the way the date is formatted in the url as a means to       obtain all of the urls for each    individual date
  a <- scan(url.text,what="",sep="\n")
  
  # search for the table
  i.table.start <- grep("<TABLE",a)
  i.table.end   <- grep("</table",a)[2]
  
  # check whether there's any movie data
  if(i.table.end-i.table.start > 5)
  {
    # find movies
    i <- grep("movie",a)
    i <- i[i>i.table.start & i<i.table.end]
    
    # get movie names
    
    data0 <- data.frame(movie=gsub("<[^>]*>","",a[i]))
    
    data0$gross <- gsub("<[^>]*>","",a[i+2])
    data0$gross <- as.numeric(gsub("[$,]","",data0$gross))
    
    # add date into the dataset
    data0$date  <- dates.list$date[i.date]
    
    results[[i.date]] <- data0
  }
  else
  {
    cat("Skipping\n")
  }
}

```

Also, after the very last bracket in your for-loop, you can place the following lines of code. First, you'll have to install the mailR package (see https://github.com/rpremraj/mailR). These lines of code will give you an email notification when the for-loop is done running. 
```{r comment="", results='hold'}
library(mailR)
send.mail(from = "gregridgeway@gmail.com",
          to = c("gregridgeway@gmail.com"),
          subject = "Movies",
          body = "R has finished downloading all the movie data",
          smtp = list(host.name="smtp.gmail.com",
                      port     =465,
                      user.name="", # add your username
                      passwd   ="", # and password
                      ssl      =TRUE),
          authenticate = TRUE,
          send = TRUE)
```

# Parallel Implementation Speeds Up the Process....

As noted above, running the above for-loop takes well over 10 minutes. If you were to webscrape additional columns or several thousand more webpages, it could probably take substantially longer. Parallel implementation can expedite the process because it uses multiple processors on your computer.

To use parallel implementation in R, you will first need to install the doParallel and foreach packages. For more information about the doParallel package, see http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).  For more information about the foreach package, see https://cran.r-project.org/web/packages/foreach/foreach.pdf.

Let's use a very basic example to see how much parallel implementation improves the speed of a for-loop. 

```{r comment="", results='hold'}
library(doParallel)
library(foreach)

cl <- makeCluster(2)
registerDoParallel(cl)

# should take 10*2=20 seconds
system.time( # time how long this takes
  foreach(i=1:10) %do% # run not in parallel
  {
     Sys.sleep(2)  # wait for 2 seconds
     return(i)
  }
)

# with two processors should take about 10 seconds
system.time( # time how long this takes
  foreach(i=1:10) %dopar% # run in parallel
  {
    Sys.sleep(2)  # wait for 2 seconds
    return(i)
  }
)

stopCluster(cl)
```

Let's web scrape the movie data using multiple processors:

```{r comment="", results='hold'}
cl <- makeCluster(8)
registerDoParallel(cl)

timeStart <- Sys.time() # record the starting time
results <-
foreach(i.date=1:nrow(dates.list)) %dopar%
{
  url.text <- paste("http://www.the-numbers.com/box-office-chart/daily/",
                    dates.list$years[i.date],"/",
                    dates.list$months[i.date],"/",
                    dates.list$days[i.date],
                    sep="")

  a <- scan(url.text,what="",sep="\n")

  # search for the table
  i.table.start <- grep("<TABLE",a)
  i.table.end   <- grep("</table",a)[2]

  # check whether there's any movie data
  if(i.table.end-i.table.start > 5)
  {
    # find movies
    i <- grep("movie",a)
    i <- i[i>i.table.start & i<i.table.end]

    # get movie names
    data0 <- data.frame(movie=gsub("<[^>]*>","",a[i]))

    # get gross
    data0$gross <- gsub("<[^>]*>","",a[i+2])
    # strip out $ and ,
    data0$gross <- gsub("[$,]","",data0$gross)
    data0$gross <- as.numeric(data0$gross)

    # add date
    data0$date  <- dates.list$date[i.date]
  }
  else
  { # if the page has no movie data
    data0 <- NULL
  }

  return(data0)
}
# calculate how long it took
timeEnd <- Sys.time()
timeEnd-timeStart

stopCluster(cl)

```

We can combine the results into one dataset. This process is the equivalent of rbind(results[[1]],results[[2]],.... that we did above. 
```{r comment="", results='hold'}
movie.data <- do.call(rbind,results)
```

Using lubridate, we can add a day of the week column:
```{r comment="", results='hold'}
movie.data$day.of.week <- wday(movie.data$date,label=TRUE)
```

You can use the dataset to answer questions such as "which movie yielded the largest gross?"
```{r comment="", results='hold'}
movie.data[which.max(movie.data$gross),]
```
Which ten movies had the largest total gross over the January 2010-October 2016 period? 
```{r comment="", results='hold'}
a <- with(movie.data, by(gross,movie,sum))
rev(sort(a))[1:10]
```
Which days of the week yielded the largest total gross?

```{r comment="", results='hold'}
with(movie.data, sort(by(gross,day.of.week,sum)))
```

Remember to save your movie.data dataframe as an .RData file. The file will be saved to the current path that you're working in. Therefore, before saving your data, it may be a good idea, using setwd(), to verify that your path is set correctly. 

```{r comment="", results='hold'}
save(movie.data,file="movie revenue.RData",compress=TRUE)
```

#Another Webscraping Exericse - Weather

We've web scraped movie gross data for a 6+ year period. We can also web scrape weather data for that same time period. We'll use the website https://www.wunderground.com, which contains historical weather information. Let's start with webscraping the July 4, 2016 weather for Chicago, Illinois:

```{r comment="", results='hold'}

library(doParallel)
library(foreach)
library(lubridate)

a <- scan("https://www.wunderground.com/history/airport/KORD/2016/7/4/DailyHistory.html",what="",sep="\n")

i <- grep("Mean Temperature",a)
a[i+2]
# remove HTML tags, HTML special characters, spaces, and F
gsub("(<[^>]*>|&[^;]*;|[ F])","",a[i+2])

```
 
We can now webscrape weather data for each date:

```{r comment="", results='hold'}
weather.data <- data.frame(date=seq(ymd("2010-01-01"),ymd("2016-10-01"),by="days"))

cl <- makeCluster(8)
registerDoParallel(cl)

# loop through all the dates
results <- foreach(i.date=1:nrow(weather.data),.packages="lubridate") %dopar%
{
   # paste together the URL
   url.text <- paste("http://www.wunderground.com/history/airport/KORD/",
      year(weather.data$date[i.date]),"/",
      month(weather.data$date[i.date]),"/",
      day(weather.data$date[i.date]),"/",
      "DailyHistory.html",
      sep="")

   # read in the webpage
   a <- scan(url.text,what="",sep="\n")

   # get the temp and precipitation data
   i <- grep("Mean Temperature",a)
   mean.temp <- gsub("(<[^>]*>|&[^;]*;|[ F])","",a[i+2])
   # the second "Precipitation"
   i <- grep("Precipitation",a)[2]
   precip <- gsub("(<[^>]*>|&[^;]*;|in)","",a[i+2])

   if(mean.temp=="-") # try Midway if O'Hare has no data
   {
      url.text <- paste("http://www.wunderground.com/history/airport/KMDW/",
            year(weather.data$date[i.date]),"/",
            month(weather.data$date[i.date]),"/",
            day(weather.data$date[i.date]),"/",
            "DailyHistory.html",
            sep="")
      a <- scan(url.text,what="",sep="\n")
      i <- grep("Mean Temperature",a)
      mean.temp <- gsub("(<[^>]*>|&[^;]*;|[ F])","",a[i+2])
   }

   return(c(mean.temp,precip))
}
results <- do.call(rbind,results)
stopCluster(cl)

weather.data$mean.temp <- as.numeric(results[,1])
weather.data$precip <- results[,2]
min(setdiff(as.numeric(results[,2]),0),na.rm=TRUE)
# change trace precipitation ("T") to 0.005
weather.data$precip[weather.data$precip==" T"] <- "0.005"
weather.data$precip <- as.numeric(weather.data$precip)

# check the values that are stored... anything strange?
sapply(weather.data,table)

# check how R is storing each variable
# mean.temp and precip are stored as character, we need numeric
sapply(weather.data,is)

# some plots
hist(weather.data$mean.temp)
hist(weather.data$precip)
plot(weather.data$mean.temp)
plot(weather.data$precip)
plot(precip~mean.temp,data=weather.data)

save(weather.data,file="weather.RData")

```

#Another Web scraping Exercise - Webscrape a Different Movie Site

We can web scrape another movie website, again using parallel implementation

```{r comment="", results='hold'}
library(doParallel)
library(foreach)
library(lubridate)

# let's generate all the dates
# use R to generate a sequence of dates (see ?seq.POSIXit)
dates.list <- data.frame(date=seq(ymd("2010-01-01"),ymd("2016-10-01"),by="days"))
dates.list$years  <- year(dates.list$date)
dates.list$months <- sprintf("%02d",month(dates.list$date))
dates.list$days   <- sprintf("%02d",day(dates.list$date))

# We're going to use a vector of data frames to collect the results
#    vector("list",[Number goes here])
#    results[[i]] will contain the movie data for date i
results <- vector("list",nrow(dates.list))

# get movie data using multiple processors
cl <- makeCluster(8)
registerDoParallel(cl)

timeStart <- Sys.time() # record the starting time
results <-
  foreach(i.date=1:nrow(dates.list)) %dopar%
  {
    cat(dates.list$date[i.date],"\n")
    
    url.text <- paste0("http://www.boxofficemojo.com/daily/chart/?view=1day&sortdate=",
                       dates.list$years[i.date],"-",
                       dates.list$months[i.date],"-",
                       dates.list$days[i.date],
                       "&p=.htm")
    
    #if(dates.list[i.date])
    a <- scan(url.text,what="",sep="\n")
    #find movies
    i <- grep("Gross To-Date",a)
    
    # try a different URL format if previous one does not work
    
    if(length(i)==0)
    {
      url.text <- paste0("http://www.boxofficemojo.com/daily/chart/?view=1day&sortdate=",
                         dates.list$years[i.date],"-",
                         dates.list$months[i.date],"-",
                         dates.list$days[i.date])
      
      a <- scan(url.text,what="",sep="\n")
      
      i <- grep("Gross To-Date",a)
    }
    
    a <- strsplit(a[i],"</tr>")
    
    # split the row into separate columns
    # </td> is the HTML tag for ending a table cell within a row
    
    a <- strsplit(a[[1]],"</td>")
    
    # eliminate the headings and footer
    
    a <- a[-c(1,length(a)-1,length(a))]
    
    # get movie names
    data0 <- data.frame(movie.names=sapply(a, function(x) 
    gsub("<[^>]*>","",x[3])))
    
    #add daily gross
    data0$gross <- sapply(a, function(x) gsub("<[^>]*>","",x[5]))
    data0$gross <- as.numeric(gsub("[$,]","", data0$gross))
    
    # add date into the dataset
    
    data0$date  <- dates.list$date[i.date]
    results[[i.date]] <- data0
  }
# calculate how long it took
timeEnd <- Sys.time()
timeEnd-timeStart

stopCluster(cl)


# combine the results into one dataset
#   equivalent to rbind(results[[1]],results[[2]],...
#   allocating and deallocating memory is expensive...this does it once
mojodata <- do.call(rbind,results)

# add day of week
mojodata$day.of.week <- wday(mojodata$date,label=TRUE)

```
